{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "\n",
    "BUCKET = 'yourbucketname'\n",
    "EXP_NAME = 'object_detection-demo' # Any valid S3 prefix.\n",
    "RUN_FULL_AL_DEMO = True # See 'Cost and Runtime' in the Markdown cell above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the bucket is in the same region as this notebook.\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client('s3')\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)['ResponseMetadata']['HTTPHeaders']['x-amz-bucket-region']\n",
    "assert bucket_region == region, \"Your S3 bucket {} and this notebook need to be in the same region.\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.manifest', 'r') as f:\n",
    "    output = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "# Shuffle output in place.\n",
    "np.random.shuffle(output)\n",
    "    \n",
    "dataset_size = len(output)\n",
    "print dataset_size\n",
    "train_test_split_index = round(dataset_size*0.8)\n",
    "train_test_split_index = int(train_test_split_index)\n",
    "print train_test_split_index\n",
    "train_data = output[:train_test_split_index]\n",
    "validation_data = output[train_test_split_index:]\n",
    "\n",
    "num_training_samples = 0\n",
    "with open('train.manifest', 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        num_training_samples += 1\n",
    "    \n",
    "with open('validation.manifest', 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll upload these manifest files to the previously defined S3 bucket so that they can be used in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3 cp train.manifest s3://{BUCKET}/{EXP_NAME}/train.manifest\n",
    "#!aws s3 cp validation.manifest s3://{BUCKET}/{EXP_NAME}/validation.manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we define S3 file paths for input and output data, the training image containing the object detection algorithm, and instantiate a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(boto3.Session().region_name, 'object-detection', repo_version='latest')\n",
    "print training_image\n",
    "augmented_manifest_filename_train = 'train.manifest'\n",
    "augmented_manifest_filename_validation = 'validation.manifest'\n",
    "bucket_name = BUCKET\n",
    "s3_prefix = EXP_NAME\n",
    "s3_output_path = 's3://{}/groundtruth-od-augmented-manifest-output'.format(bucket_name) # Replace with your desired output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines paths for use in the training job request.\n",
    "s3_train_data_path = 's3://{}/{}/{}'.format(bucket_name, s3_prefix, augmented_manifest_filename_train)\n",
    "s3_validation_data_path = 's3://{}/{}/{}'.format(bucket_name, s3_prefix, augmented_manifest_filename_validation)\n",
    "\n",
    "print(\"Augmented manifest for training data: {}\".format(s3_train_data_path))\n",
    "print(\"Augmented manifest for validation data: {}\".format(s3_validation_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples = 53\n",
    "\n",
    "print num_training_samples\n",
    "#change recycle-bounding-boxes to appropriate attribute from manifest file\n",
    "attribute_names=['source-ref', 'recycle-bounding-boxes']\n",
    "# Determine the keys in the training manifest and exclude the meta data from the labling job.\n",
    "print attribute_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if attribute_names == [\"source-ref\", \"XXXX\"]:\n",
    "        raise Exception(\"The 'attribute_names' variable is set to default values. Please check your augmented manifest file for the label attribute name and set the 'attribute_names' variable accordingly.\")\n",
    "except NameError:\n",
    "    raise Exception(\"The attribute_names variable is not defined. Please check your augmented manifest file for the label attribute name and set the 'attribute_names' variable accordingly.\")\n",
    "\n",
    "# Create unique job name\n",
    "job_name_prefix = 'ground-truthod-demo'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_params = \\\n",
    "    {\n",
    "        \"AlgorithmSpecification\": {\n",
    "            # NB. This is one of the named constants defined in the first cell.\n",
    "            \"TrainingImage\": training_image,\n",
    "            \"TrainingInputMode\": \"Pipe\"\n",
    "        },\n",
    "        \"RoleArn\": role,\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": s3_output_path\n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.p3.16xlarge\",\n",
    "            \"VolumeSizeInGB\": 100\n",
    "        },\n",
    "        \"TrainingJobName\": model_job_name,\n",
    "        \"HyperParameters\": {  # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n",
    "            \"base_network\": \"resnet-50\",\n",
    "            \"use_pretrained_model\": \"0\",\n",
    "            \"num_classes\": \"1\",\n",
    "            \"mini_batch_size\": \"10\",\n",
    "            \"epochs\": \"150\",\n",
    "            \"learning_rate\": \"0.001\",\n",
    "            \"lr_scheduler_step\": \"\",\n",
    "            \"lr_scheduler_factor\": \"0.1\",\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"momentum\": \"0.9\",\n",
    "            \"weight_decay\": \"0.0005\",\n",
    "            \"overlap_threshold\": \"0.5\",\n",
    "            \"nms_threshold\": \"0.45\",\n",
    "            \"image_shape\": \"852\",\n",
    "            \"label_width\": \"350\",\n",
    "            \"num_training_samples\": str(num_training_samples)\n",
    "        },\n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 86400\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
    "                        \"S3Uri\": s3_train_data_path,\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                        # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                        \"AttributeNames\": attribute_names\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"application/x-recordio\",\n",
    "                \"RecordWrapperType\": \"RecordIO\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"validation\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
    "                        \"S3Uri\": s3_validation_data_path,\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                        # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                        \"AttributeNames\": attribute_names\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"application/x-recordio\",\n",
    "                \"RecordWrapperType\": \"RecordIO\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "print('Training job name: {}'.format(model_job_name))\n",
    "print('\\nInput Data Location: {}'.format(\n",
    "    training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "status = client.describe_training_job(TrainingJobName=model_job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progess of the training job, you can repeatedly evaluate the following cell. When the training job status reads `'Completed'`, move on to the next part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(service_name='sagemaker')\n",
    "print(\"Training job status: \", client.describe_training_job(TrainingJobName=model_job_name)['TrainingJobStatus'])\n",
    "print(\"Secondary status: \", client.describe_training_job(TrainingJobName=model_job_name)['SecondaryStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_info = client.describe_training_job(TrainingJobName=model_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model \n",
    "\n",
    "Now that we've fully labeled our dataset and have a trained model, we want to use the model to perform inference.\n",
    "\n",
    "Object detection only supports encoded .jpg and .png image formats as inference input for now. The output is in JSON format, or in JSON Lines format for batch transform.\n",
    "\n",
    "This section involves several steps:\n",
    "1. Create Model: Create model for the training output\n",
    "2. Batch Transform: Create a transform job to perform batch inference.\n",
    "3. Host the model for realtime inference: Create an inference endpoint and perform realtime inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name='groundtruth-demo-od-model' + timestamp\n",
    "\n",
    "print(model_name)\n",
    "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sagemaker_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realtime Inference\n",
    "\n",
    "We now host the model with an endpoint and perform realtime inference.\n",
    "\n",
    "This section involves several steps:\n",
    "\n",
    "1. Create endpoint configuration - Create a configuration defining an endpoint.\n",
    "2. Create endpoint - Use the configuration to create an inference endpoint.\n",
    "3. Perform inference - Perform inference on some input data using the endpoint.\n",
    "4. Clean up - Delete the endpoint and model\n",
    "\n",
    "### Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-epc' + timestamp\n",
    "endpoint_config_response = sagemaker_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "The next cell creates an endpoint that can be validated and incorporated into production applications. This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-ep' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker_client.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "# get the status of the endpoint\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker_client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference\n",
    "\n",
    "The following cell transforms the image into the appropriate format for realtime prediction, submits the job, receives the prediction from the endpoint, and plots the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_detection(img_file, dets, classes=[], thresh=0.6):\n",
    "        \"\"\"\n",
    "        visualize detections in one image\n",
    "        Parameters:\n",
    "        ----------\n",
    "        img : numpy.array\n",
    "            image, in bgr format\n",
    "        dets : numpy.array\n",
    "            ssd detections, numpy.array([[id, score, x1, y1, x2, y2]...])\n",
    "            each row is one object\n",
    "        classes : tuple or list of str\n",
    "            class names\n",
    "        thresh : float\n",
    "            score threshold\n",
    "        \"\"\"\n",
    "        import random\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.image as mpimg\n",
    "\n",
    "        img=mpimg.imread(img_file)\n",
    "        plt.imshow(img)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        colors = dict()\n",
    "        for det in dets:\n",
    "            (klass, score, x0, y0, x1, y1) = det\n",
    "            if score < thresh:\n",
    "                continue\n",
    "            cls_id = int(klass)\n",
    "            if cls_id not in colors:\n",
    "                colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "            xmin = int(x0 * width)\n",
    "            ymin = int(y0 * height)\n",
    "            xmax = int(x1 * width)\n",
    "            ymax = int(y1 * height)\n",
    "            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                 ymax - ymin, fill=False,\n",
    "                                 edgecolor=colors[cls_id],\n",
    "                                 linewidth=3.5)\n",
    "            plt.gca().add_patch(rect)\n",
    "            class_name = str(cls_id)\n",
    "            if classes and len(classes) > cls_id:\n",
    "                class_name = classes[cls_id]\n",
    "            plt.gca().text(xmin, ymin - 2,\n",
    "                            '{:s} {:.3f}'.format(class_name, score),\n",
    "                            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                                    fontsize=12, color='white')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "file_name =\"test2.png\"\n",
    "\n",
    "#test_image =\"vid2_12.png\" test_image =\"vid3_15.png\"\n",
    "\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = bytearray(payload)\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "\n",
    "results = response['Body'].read()\n",
    "detections = json.loads(results)\n",
    "\n",
    "\n",
    "print (results)\n",
    "\n",
    "object_categories = ['paper','plastic','cardboard','metal','glass','trash']\n",
    "\n",
    "threshold = 0.65\n",
    "\n",
    "visualize_detection(file_name, detections['prediction'], object_categories, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "Finally, let's clean up and delete this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boto3.client(service_name='sagemaker').delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "We covered a lot of ground in this notebook! Let's recap what we accomplished. First we started with an unlabeled dataset (technically, the dataset was previously labeled by the authors of the dataset, but we discarded the original labels for the purposes of this demonstration). Next, we created a SageMake Ground Truth labeling job and generated new labels for all of the images in our dataset. Then we split this file into a training set and a validation set and trained a SageMaker object detection model. Next, we trained a new model using these Ground Truth results and submitted a batch job to label a held-out image from the original dataset. Finally, we created a hosted model endpoint and used it to make a live prediction for the same held-out image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
